import streamlit as st
import pandas as pd
import random
import requests
from collections import Counter
import re
from deep_translator import GoogleTranslator
import matplotlib
import matplotlib.pyplot as plt

# ğŸ› ï¸ å¼ºåˆ¶åå°ç”»å›¾ (é˜²ç™½å±)
matplotlib.use('Agg')

# --- 1. é¡µé¢é«˜çº§é…ç½® ---
st.set_page_config(page_title="Ozon é€‰å“é›·è¾¾ (æ——èˆ°ç‰ˆ)", page_icon="ğŸš€", layout="wide")
st.markdown("""
    <style>
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    .stMetric {
        background-color: #f0f2f6;
        padding: 15px;
        border-radius: 10px;
        text-align: center;
    }
    </style>
    """, unsafe_allow_html=True)

# --- 2. æ ¸å¿ƒåŠŸèƒ½ç±» ---
class OzonAnalyzer:
    def __init__(self):
        self.translator = GoogleTranslator(source='auto', target='zh-CN')

    def translate(self, text):
        try:
            return self.translator.translate(text)
        except:
            return text

    def get_real_data(self, keyword):
        api_key = st.secrets.get("RAPIDAPI_KEY", "")
        if not api_key or "YOUR" in api_key: return None 

        url = "https://ozon-scraper-api.p.rapidapi.com/v1/search"
        headers = {"X-RapidAPI-Key": api_key, "X-RapidAPI-Host": "ozon-scraper-api.p.rapidapi.com"}
        
        try:
            response = requests.get(url, headers=headers, params={"text": keyword, "page": "1"}, timeout=15)
            if response.status_code != 200: return None
            
            data = response.json()
            items = []
            for item in data.get('items', []):
                price = item.get('price', {}).get('amount', 0)
                if price == 0: price = item.get('price_rub', 0)
                
                title = item.get('title', 'æœªçŸ¥')
                rating = float(item.get('rating', {}).get('average', 0.0))
                reviews = int(item.get('rating', {}).get('count', 0))
                
                items.append({
                    "title_origin": title,
                    "price_rub": float(price),
                    "reviews": reviews,
                    "rating": rating,
                    "link": item.get('url', f"https://www.ozon.ru/search/?text={keyword}"),
                    "is_real": True
                })
            return items
        except:
            return None

    def get_mock_data(self, keyword):
        data = []
        base = random.randint(500, 3000)
        # æ¨¡æ‹Ÿæ›´çœŸå®çš„åˆ†å¸ƒ
        for i in range(30):
            price = max(100, base + random.randint(-500, 1500))
            reviews = random.randint(0, 100) if random.random() > 0.2 else random.randint(500, 3000)
            data.append({
                "title_origin": f"[æ¨¡æ‹Ÿ] {keyword} æ ·å¼{chr(65+i)} Pro Max",
                "price_rub": price,
                "reviews": reviews,
                "rating": round(random.uniform(3.0, 5.0), 1),
                "link": "https://www.ozon.ru",
                "is_real": False
            })
        return data

# --- 3. è¾…åŠ©åˆ†æå‡½æ•° ---
def extract_keywords(titles):
    # ç®€å•çš„åˆ†è¯ç»Ÿè®¡
    text = " ".join(titles).lower()
    # å»æ‰æ ‡ç‚¹å’Œæ— æ„ä¹‰è¯
    text = re.sub(r'[^\w\s]', '', text)
    words = text.split()
    stop_words = {'the', 'for', 'and', 'with', 'ozon', 'æ¨¡æ‹Ÿ', 'pro', 'max', 'new', 'set', 'of'}
    filtered = [w for w in words if w not in stop_words and len(w) > 2]
    return Counter(filtered).most_common(10)

# --- 4. ç•Œé¢é€»è¾‘ ---
st.title("ğŸš€ Ozon é€‰å“é›·è¾¾ (æ——èˆ°ç‰ˆ V2.0)")

# ä¾§è¾¹æ ï¼šå‚æ•°è®¾ç½®
with st.sidebar:
    st.header("âš™ï¸ å‚æ•°é…ç½®")
    keyword = st.text_input("ğŸ” æœç´¢å…³é”®è¯", "crochet bag")
    st.markdown("---")
    st.header("ğŸ’° åˆ©æ¶¦æ¨¡å‹")
    ex_rate = st.number_input("æ±‡ç‡ (CNY/RUB)", 0.075, format="%.4f")
    cost_cny = st.number_input("é‡‡è´­æˆæœ¬ (Â¥)", 40.0)
    fee = st.slider("å¹³å°ä½£é‡‘ (%)", 10, 40, 15) / 100
    st.markdown("---")
    st.caption("Developed by Gemini AI Partner")

# ä¸»æŒ‰é’®
if st.button("ğŸš€ å…¨ç½‘æ·±åº¦æŒ–æ˜", type="primary", use_container_width=True):
    
    analyzer = OzonAnalyzer()
    
    with st.spinner("ğŸ•µï¸â€â™‚ï¸ AI æ­£åœ¨çˆ¬å–æ•°æ®ã€æ¸…æ´—å™ªéŸ³ã€è®¡ç®—åˆ©æ¶¦..."):
        # 1. è·å–æ•°æ®
        raw = analyzer.get_real_data(keyword)
        if not raw:
            raw = analyzer.get_mock_data(keyword)
            is_mock = True
        else:
            is_mock = False
            
        df = pd.DataFrame(raw)

        # 2. è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
        df['ä»·æ ¼ (Â¥)'] = df['price_rub'] * ex_rate
        df['å‡€åˆ©æ¶¦ (Â¥)'] = df['ä»·æ ¼ (Â¥)'] * (1 - fee) - cost_cny
        df['ROI (%)'] = (df['å‡€åˆ©æ¶¦ (Â¥)'] / cost_cny) * 100
        
        # 3. æ™ºèƒ½è¯„åˆ†
        df['çˆ†æ¬¾åˆ†'] = 0
        df.loc[df['ROI (%)'] > 30, 'çˆ†æ¬¾åˆ†'] += 40
        df.loc[df['reviews'] > 100, 'çˆ†æ¬¾åˆ†'] += 30
        df.loc[df['rating'] > 4.2, 'çˆ†æ¬¾åˆ†'] += 30

        # 4. ç¿»è¯‘
        df['ä¸­æ–‡æ ‡é¢˜'] = df['title_origin'].apply(analyzer.translate)
        df = df.sort_values("çˆ†æ¬¾åˆ†", ascending=False)

    # === ğŸŸ¢ æ¨¡å— 1: å¸‚åœºå¤§ç›˜ä»ªè¡¨æ¿ ===
    st.divider()
    if is_mock:
        st.warning("âš ï¸ å½“å‰ä¸ºæ¼”ç¤ºæ•°æ®æ¨¡å¼ (API æœªè¿æ¥)")
    else:
        st.success(f"âœ… æˆåŠŸæŠ“å– {len(df)} æ¡çœŸå®ç«å“æ•°æ®")

    # 4ä¸ªæ ¸å¿ƒæŒ‡æ ‡å¡ç‰‡
    m1, m2, m3, m4 = st.columns(4)
    m1.metric("å¹³å‡å”®ä»· (å¢å¸ƒ)", f"â‚½{int(df['price_rub'].mean())}")
    m2.metric("å¸‚åœºå¹³å‡ ROI", f"{int(df['ROI (%)'].mean())}%", delta_color="normal")
    m3.metric("å¤´éƒ¨ç«å“æœ€é«˜é”€", f"{df['reviews'].max()} æ¡")
    m4.metric("ç›ˆåˆ©å•†å“å æ¯”", f"{len(df[df['å‡€åˆ©æ¶¦ (Â¥)']>0]) / len(df) * 100:.0f}%")

    # === ğŸ”µ æ¨¡å— 2: å¤šç»´åº¦åˆ†æ Tabs ===
    tab1, tab2, tab3 = st.tabs(["ğŸ“‹ é€‰å“çŸ©é˜µè¡¨", "ğŸ“Š å¯è§†åŒ–å›¾è¡¨", "ğŸ§  SEO å…³é”®è¯åŠ©æ‰‹"])

    with tab1:
        st.subheader("å…¨é‡å•†å“åˆ©æ¶¦åˆ†æ")
        
        # è¿‡æ»¤å™¨
        c1, c2 = st.columns(2)
        min_roi = c1.slider("è¿‡æ»¤ ROI ä½äºå¤šå°‘çš„äº§å“?", 0, 100, 0)
        show_df = df[df['ROI (%)'] >= min_roi]
        
        # æ¸²æŸ“çƒ­åŠ›å›¾
        st.dataframe(
            show_df.style.background_gradient(subset=['çˆ†æ¬¾åˆ†', 'å‡€åˆ©æ¶¦ (Â¥)', 'ROI (%)'], cmap="RdYlGn"),
            column_config={
                "ä¸­æ–‡æ ‡é¢˜": st.column_config.TextColumn("å•†å“", width="medium"),
                "price_rub": st.column_config.NumberColumn("å¢å¸ƒä»·", format="â‚½%d"),
                "å‡€åˆ©æ¶¦ (Â¥)": st.column_config.NumberColumn("å‡€åˆ©", format="Â¥%.1f"),
                "ROI (%)": st.column_config.NumberColumn("ROI", format="%.0f%%"),
                "link": st.column_config.LinkColumn("é“¾æ¥"),
            },
            use_container_width=True,
            hide_index=True
        )
        
        # ğŸ“¥ ä¸‹è½½ Excel åŠŸèƒ½
        csv = show_df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½æ•°æ®åˆ° Excel",
            data=csv,
            file_name=f'ozon_analysis_{keyword}.csv',
            mime='text/csv',
        )

    with tab2:
        st.subheader("å¸‚åœºåˆ†å¸ƒé€è§†")
        col_chart1, col_chart2 = st.columns(2)
        
        with col_chart1:
            st.markdown("**ğŸ’° ä»·æ ¼åŒºé—´åˆ†å¸ƒ (å“ªä¸ªä»·ä½æ®µäº§å“æœ€å¤š?)**")
            # ä»·æ ¼ç›´æ–¹å›¾
            st.bar_chart(df['price_rub'].value_counts(bins=5).sort_index())
            
        with col_chart2:
            st.markdown("**ğŸ’ ä»·æ ¼ vs è¯„ä»·æ•° (å¯»æ‰¾ä»·æ ¼é«˜ä¸”è¯„ä»·å°‘çš„è“æµ·)**")
            # æ•£ç‚¹å›¾
            st.scatter_chart(df, x='price_rub', y='reviews', color='ROI (%)')

    with tab3:
        st.subheader("ğŸ”‘ çˆ†æ¬¾æ ‡é¢˜é«˜é¢‘è¯ (SEO)")
        st.caption("å°†è¿™äº›è¯åŠ å…¥ä½ çš„æ ‡é¢˜ï¼Œæ›´å®¹æ˜“è¢«ä¹°å®¶æœç´¢åˆ°")
        
        keywords = extract_keywords(df['title_origin'].tolist())
        kw_df = pd.DataFrame(keywords, columns=['å•è¯', 'å‡ºç°æ¬¡æ•°'])
        
        # æ¨ªå‘æŸ±çŠ¶å›¾å±•ç¤ºå…³é”®è¯
        st.bar_chart(kw_df.set_index('å•è¯'))
        
        with st.expander("æŸ¥çœ‹æ¨èæ ‡é¢˜ç»„åˆ"):
            top_words = [k[0] for k in keywords[:5]]
            st.write(f"ğŸ¤– **AI æ¨èç»„åˆ**: {keyword} " + " ".join(top_words))
